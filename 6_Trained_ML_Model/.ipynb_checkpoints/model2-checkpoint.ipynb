{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a80000c-cd55-41c5-aa3d-3a624cf4d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchmetrics import Accuracy\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e02743c4-4f60-497e-9a1a-5081b2c083db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/upadesh/3_Codes/6_Au_Au_Laser/1_Simulation_Results/ML_numpy_files'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path = os.path.abspath('data/')\n",
    "path = os.path.abspath('../1_Simulation_Results/ML_numpy_files/')\n",
    "\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb3a88e-b813-4aab-b95c-eb0742a67e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = np.load(path+'/X.npy'), np.load(path+'/y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8a7f3c6-1a8a-4dd8-b508-a420eecc6eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((406, 201, 401), (406, 201, 401))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a1b05e3-b634-4535-a411-53a22dc57811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape X and y to match PyTorch's Conv2d input format: (batch_size, channels, height, width)\n",
    "X_reshaped = X[:, np.newaxis, :, :]  # Shape: (406, 1, 201, 401)\n",
    "y_reshaped = y[:, np.newaxis, :, :]  # Shape: (406, 1, 201, 401)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_, X_test, y_, y_test = train_test_split(X_reshaped, y_reshaped, test_size=0.1, random_state=69)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.11, random_state=69)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1cafe8f-98cb-4c06-9d83-01dd014aed7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature [batch, channel, width, height] => Train: torch.Size([324, 1, 201, 401]) | Test: torch.Size([41, 1, 201, 401]) | Validation torch.Size([41, 1, 201, 401])\n",
      "Label   [batch, channel, width, height] => Train: torch.Size([324, 1, 201, 401]) | Test: torch.Size([41, 1, 201, 401]) | Validation torch.Size([41, 1, 201, 401])\n"
     ]
    }
   ],
   "source": [
    "print(f'Feature [batch, channel, width, height] => Train: {X_train.shape} | Test: {X_test.shape} | Validation {X_val.shape}')\n",
    "print(f'Label   [batch, channel, width, height] => Train: {y_train.shape} | Test: {y_test.shape} | Validation {y_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aeecf7-22c9-45cb-8627-e16565ec6bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c2717b-99d4-426b-80b7-c0abd74e8d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        def conv_block(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        \n",
    "        self.encoder1 = conv_block(in_channels, 64)\n",
    "        self.encoder2 = conv_block(64, 128)\n",
    "        self.encoder3 = conv_block(128, 256)\n",
    "        self.encoder4 = conv_block(256, 512)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.middle = conv_block(512, 1024)\n",
    "        \n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.decoder4 = conv_block(1024, 512)\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = conv_block(512, 256)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = conv_block(256, 128)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = conv_block(128, 64)\n",
    "        \n",
    "        self.final = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool(enc1))\n",
    "        enc3 = self.encoder3(self.pool(enc2))\n",
    "        enc4 = self.encoder4(self.pool(enc3))\n",
    "        \n",
    "        middle = self.middle(self.pool(enc4))\n",
    "        \n",
    "        dec4 = self.upconv4(middle)\n",
    "        dec4 = self.pad_and_crop(enc4, dec4)\n",
    "        dec4 = torch.cat((dec4, dec4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        \n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = self.pad_and_crop(enc3, dec3)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        \n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = self.pad_and_crop(enc2, dec2)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        \n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = self.pad_and_crop(enc1, dec1)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        \n",
    "        out = self.final(dec1)\n",
    "        return out\n",
    "    \n",
    "    def pad_and_crop(self, target, tensor):\n",
    "        \"\"\"Pad the tensor and crop to match the target dimensions.\"\"\"\n",
    "        _, _, target_height, target_width = target.size()\n",
    "        _, _, tensor_height, tensor_width = tensor.size()\n",
    "        \n",
    "        # Padding\n",
    "        pad_h = max(0, target_height - tensor_height)\n",
    "        pad_w = max(0, target_width - tensor_width)\n",
    "        \n",
    "        pad_left = pad_w // 2\n",
    "        pad_right = pad_w - pad_left\n",
    "        pad_top = pad_h // 2\n",
    "        pad_bottom = pad_h - pad_top\n",
    "        \n",
    "        tensor = F.pad(tensor, (pad_left, pad_right, pad_top, pad_bottom), mode='constant', value=0)\n",
    "        \n",
    "        # Cropping\n",
    "        delta_h = (tensor.size(2) - target_height) // 2\n",
    "        delta_w = (tensor.size(3) - target_width) // 2\n",
    "        \n",
    "        return tensor[:, :, delta_h: delta_h + target_height, delta_w: delta_w + target_width]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11600107-18a9-4aaa-8199-383a82a17b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return feature, label\n",
    "\n",
    "# Example usage:\n",
    "# features = np.random.randn(406, 1, 201, 401)  # Replace with your feature data\n",
    "# labels = np.random.randint(0, 2, (406, 1, 201, 401))  # Replace with your label data\n",
    "\n",
    "dataset = CustomDataset(X_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b2c3c-929b-4156-8c65-43e64a732481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet(in_channels=1, out_channels=1).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Example training loop\n",
    "for epoch in range(10):  # Adjust number of epochs\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataset)\n",
    "    print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e7cd34-cb0f-4276-88e2-9c0a1506f48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94096a0-19a8-4115-bc6a-af53e8f168a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'Unet_model.pth')\n",
    "print('Model saved successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d8ea1f-fd8e-418b-a9c9-f0b06081c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire model\n",
    "model = torch.load('Unet_model.pth')\n",
    "model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008327b-38e9-4160-8ff1-53f12d8b07ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "# Binary predictions\n",
    "all_preds = (all_preds > 0.5).astype(np.float32)\n",
    "\n",
    "# Example evaluation metric\n",
    "accuracy = accuracy_score(all_targets.flatten(), all_preds.flatten())\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a7e56c-f1e7-4321-8960-3ed7eb888fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eea1c35-e4bb-4c61-845d-c1dc9e15bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Assuming your test data is stored in test_features and has shape (num_samples, 1, 201, 401)\n",
    "# Replace test_features with your actual test data array\n",
    "test_features = X_test[:,:,:,:]  # Example data, replace with actual test data\n",
    "\n",
    "# Convert the test features to PyTorch tensors\n",
    "test_tensor = torch.tensor(test_features, dtype=torch.float32)\n",
    "\n",
    "# Create a DataLoader for your test set\n",
    "test_dataset = TensorDataset(test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move model to the correct device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Store predictions\n",
    "all_predictions = []\n",
    "\n",
    "# Disable gradient computation for testing\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        inputs = inputs[0].to(device)  # Get the input tensor and move it to the device\n",
    "        \n",
    "        # Forward pass to get the output\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Apply sigmoid to get probabilities if using BCEWithLogitsLoss\n",
    "        predictions = torch.sigmoid(outputs)\n",
    "        \n",
    "        # Convert probabilities to binary predictions (0 or 1)\n",
    "        predicted_labels = (predictions > 0.5).float()  # Threshold at 0.5\n",
    "        \n",
    "        # Move predictions back to the CPU and convert to numpy\n",
    "        all_predictions.append(predicted_labels.cpu().numpy())\n",
    "\n",
    "# Concatenate all predictions\n",
    "all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "\n",
    "# Output predictions\n",
    "print(\"Predictions shape:\", all_predictions.shape)\n",
    "# print(all_predictions)\n",
    "\n",
    "error = all_predictions - np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f3db19-11b9-405a-b8e8-2496617f98f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df695e01-25c3-47d3-a783-33336031a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_step = 20\n",
    "# laser_pos = (125 + time[t_step]*laser_speed)* 401/1000  # Laser actual position in true dimension\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3,1, figsize=(12,12), frameon=True)\n",
    "cmap = plt.get_cmap('RdYlGn_r')\n",
    "cmap.set_under('white', alpha=0)\n",
    "hmap1 = ax1.imshow(all_predictions[t_step][0], cmap=cmap, vmin=0.5, vmax=1.0, aspect=0.5,  interpolation='quadric')\n",
    "cmap = plt.get_cmap('Wistia')\n",
    "cmap.set_under('white', alpha=0) \n",
    "hmap2 = ax1.imshow(1-all_predictions[t_step][0], cmap=cmap, vmin=0.5, vmax=1.5, aspect=0.5, interpolation='quadric')\n",
    "\n",
    "\n",
    "cmap = plt.get_cmap('RdYlGn_r')\n",
    "cmap.set_under('white', alpha=0)\n",
    "hmap1 = ax2.imshow(y_test[t_step][0] , cmap=cmap, vmin=0.5, vmax=1.0, aspect=0.5,  interpolation='quadric')\n",
    "cmap = plt.get_cmap('Wistia')\n",
    "cmap.set_under('white', alpha=0) \n",
    "hmap2 = ax2.imshow(1-y_test[t_step][0], cmap=cmap, vmin=0.5, vmax=1.5, aspect=0.5, interpolation='quadric')\n",
    "\n",
    "cmap = plt.get_cmap('RdYlGn_r')\n",
    "cmap.set_under('white', alpha=0)\n",
    "hmap1 = ax3.imshow(error[t_step][0], cmap=cmap, aspect=0.5,  interpolation='quadric')\n",
    "cmap = plt.get_cmap('Wistia')\n",
    "cmap.set_under('white', alpha=0) \n",
    "hmap2 = ax3.imshow(1-error[t_step][0], cmap=cmap, aspect=0.5, interpolation='quadric')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da2d54-683b-4dec-8080-b69026f14949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24fc543-20af-40d9-bb30-935a5ba1d5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31b6f6-92c7-4a32-8270-5b538d489f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6add485-54cd-4372-aeff-a9ecd9ce9136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef41bcdf-c424-48da-a47c-a90437fba3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# U-Net Model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Encoder: Contracting path\n",
    "        self.enc_conv1 = self.conv_block(1, 64)\n",
    "        self.enc_conv2 = self.conv_block(64, 128)\n",
    "        self.enc_conv3 = self.conv_block(128, 256)\n",
    "        self.enc_conv4 = self.conv_block(256, 512)\n",
    "        \n",
    "        # Decoder: Expanding path\n",
    "        self.upconv3 = self.upconv_block(512, 256)\n",
    "        self.upconv2 = self.upconv_block(512, 128)  # Adjusted channels\n",
    "        self.upconv1 = self.upconv_block(256, 64)   # Adjusted channels\n",
    "        \n",
    "        # Final output layer\n",
    "        self.out_conv = nn.Conv2d(128, 1, kernel_size=1)  # Adjusted channels\n",
    "    \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def crop_and_concat(self, enc_output, dec_output):\n",
    "        \"\"\"\n",
    "        Crops enc_output to match the size of dec_output, and then concatenates along the channel axis.\n",
    "        \"\"\"\n",
    "        enc_output_size = enc_output.size()[2:]  # Get H, W of encoder output\n",
    "        dec_output_size = dec_output.size()[2:]  # Get H, W of decoder output\n",
    "        \n",
    "        # Compute the difference in sizes (crop to match decoder size)\n",
    "        crop_h = (enc_output_size[0] - dec_output_size[0]) // 2\n",
    "        crop_w = (enc_output_size[1] - dec_output_size[1]) // 2\n",
    "        \n",
    "        enc_output_cropped = enc_output[:, :, crop_h:enc_output_size[0] - crop_h, crop_w:enc_output_size[1] - crop_w]\n",
    "        \n",
    "        # In case of odd differences, crop one more pixel from the bottom/right\n",
    "        if enc_output_cropped.size()[2] != dec_output.size()[2]:\n",
    "            enc_output_cropped = enc_output_cropped[:, :, :-1, :]\n",
    "        if enc_output_cropped.size()[3] != dec_output.size()[3]:\n",
    "            enc_output_cropped = enc_output_cropped[:, :, :, :-1]\n",
    "        \n",
    "        return torch.cat([enc_output_cropped, dec_output], dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        enc1 = self.enc_conv1(x)   # Output: (batch_size, 64, 201, 401)\n",
    "        enc2 = self.enc_conv2(F.max_pool2d(enc1, 2))  # Output: (batch_size, 128, 100, 200)\n",
    "        enc3 = self.enc_conv3(F.max_pool2d(enc2, 2))  # Output: (batch_size, 256, 50, 100)\n",
    "        enc4 = self.enc_conv4(F.max_pool2d(enc3, 2))  # Output: (batch_size, 512, 25, 50)\n",
    "        \n",
    "        # Decoding\n",
    "        dec3 = self.upconv3(enc4)  # Output: (batch_size, 256, 50, 100)\n",
    "        dec3 = self.crop_and_concat(enc3, dec3)  # Concatenate with enc3 (batch_size, 512, 50, 100)\n",
    "        \n",
    "        dec2 = self.upconv2(dec3)  # Output: (batch_size, 128, 100, 200)\n",
    "        dec2 = self.crop_and_concat(enc2, dec2)  # Concatenate with enc2 (batch_size, 256, 100, 200)\n",
    "        \n",
    "        dec1 = self.upconv1(dec2)  # Output: (batch_size, 64, 201, 401)\n",
    "        dec1 = self.crop_and_concat(enc1, dec1)  # Concatenate with enc1 (batch_size, 128, 201, 401)\n",
    "        \n",
    "        # Output\n",
    "        out = self.out_conv(dec1)  # Output: (batch_size, 1, 201, 401)\n",
    "        return torch.sigmoid(out)  # Binary classification (pixel-wise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b30af71-02ea-464b-9722-48ca2b3291d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89d29b-b09a-4ed8-a15c-e4e195dbe76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Device configuration\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # Initialize model\n",
    "# model = UNet().to(device)\n",
    "\n",
    "# # Loss and optimizer\n",
    "# criterion = nn.BCELoss()  # Binary Cross-Entropy loss\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# X_train = X_train.to(device)  # Move input tensor to GPU if available\n",
    "# y_train = y_train.to(device)  \n",
    "\n",
    "\n",
    "# # Train the model\n",
    "# epochs = 10\n",
    "# batch_size = 2\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for i in range(0, len(X_train), batch_size):\n",
    "#         X_batch = X_train[i:i+batch_size]\n",
    "#         y_batch = y[i:i+batch_size]\n",
    "        \n",
    "#         # Forward pass\n",
    "#         outputs = model(X_batch)\n",
    "#         loss = criterion(outputs, y_batch)\n",
    "        \n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "    \n",
    "#     print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(X_train):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5092e1f-78cd-4c2d-a4ad-af513d843ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8062ea-109f-43b4-8657-9fb6e30dd128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce0b65a-50f5-4589-981d-7065328798c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4560f6b1-0f23-46ac-91b0-ea1eff360cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc_conv1 = self.conv_block(1, 64)\n",
    "        self.enc_conv2 = self.conv_block(64, 128)\n",
    "        self.enc_conv3 = self.conv_block(128, 256)\n",
    "        self.enc_conv4 = self.conv_block(256, 512)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up_conv4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec_conv4 = self.conv_block(512, 256)\n",
    "        \n",
    "        self.up_conv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec_conv3 = self.conv_block(256, 128)\n",
    "        \n",
    "        self.up_conv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec_conv2 = self.conv_block(128, 64)\n",
    "        \n",
    "        self.up_conv1 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)\n",
    "        self.dec_conv1 = self.conv_block(128, 64)\n",
    "        \n",
    "        # Output layer\n",
    "        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)  # Output channels: 1 for binary classification\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        enc1 = self.enc_conv1(x)\n",
    "        enc2 = self.enc_conv2(self.pool(enc1))\n",
    "        enc3 = self.enc_conv3(self.pool(enc2))\n",
    "        enc4 = self.enc_conv4(self.pool(enc3))\n",
    "        \n",
    "        print(f'Encoder feature sizes: {enc1.size()}, {enc2.size()}, {enc3.size()}, {enc4.size()}')\n",
    "        \n",
    "        # Decoder path\n",
    "        dec4 = self.up_conv4(enc4)\n",
    "        dec4 = self.crop_and_concat(enc3, dec4)\n",
    "        dec4 = self.dec_conv4(dec4)\n",
    "        \n",
    "        dec3 = self.up_conv3(dec4)\n",
    "        dec3 = self.crop_and_concat(enc2, dec3)\n",
    "        dec3 = self.dec_conv3(dec3)\n",
    "        \n",
    "        dec2 = self.up_conv2(dec3)\n",
    "        dec2 = self.crop_and_concat(enc1, dec2)\n",
    "        dec2 = self.dec_conv2(dec2)\n",
    "        \n",
    "        dec1 = self.up_conv1(dec2)\n",
    "        dec1 = self.crop_and_concat(x, dec1)\n",
    "        dec1 = self.dec_conv1(dec1)\n",
    "        \n",
    "        print(f'Decoder feature sizes: {dec4.size()}, {dec3.size()}, {dec2.size()}, {dec1.size()}')\n",
    "        \n",
    "        return torch.sigmoid(self.final_conv(dec1))  # Output activation for binary classification\n",
    "    \n",
    "    def crop_and_concat(self, enc_feature, dec_feature):\n",
    "        \"\"\"\n",
    "        Crop the encoder feature map to match the size of the decoder feature map and concatenate.\n",
    "        \"\"\"\n",
    "        enc_size = enc_feature.size()\n",
    "        dec_size = dec_feature.size()\n",
    "        print(f'Cropping: enc_feature size {enc_size}, dec_feature size {dec_size}')\n",
    "        if enc_size[2] > dec_size[2] or enc_size[3] > dec_size[3]:\n",
    "            # Center-crop the encoder feature map\n",
    "            start_x = (enc_size[3] - dec_size[3]) // 2\n",
    "            start_y = (enc_size[2] - dec_size[2]) // 2\n",
    "            enc_feature = enc_feature[:, :, start_y:start_y + dec_size[2], start_x:start_x + dec_size[3]]\n",
    "        elif enc_size[2] < dec_size[2] or enc_size[3] < dec_size[3]:\n",
    "            # Pad the encoder feature map\n",
    "            pad_x = (dec_size[3] - enc_size[3]) // 2\n",
    "            pad_y = (dec_size[2] - enc_size[2]) // 2\n",
    "            enc_feature = nn.functional.pad(enc_feature, (pad_x, pad_x, pad_y, pad_y))\n",
    "        return torch.cat((enc_feature, dec_feature), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0e3f6-2c15-4b66-a95f-b3f49842180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bed5e2-c479-48c8-9ee6-23c411b9e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Instantiate and move the model to the appropriate device\n",
    "model = UNet().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "accumulation_steps = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (X_batch, y_batch) in enumerate(data_loader):\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(data_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss/len(data_loader)}')\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a216eafb-e7d1-4d65-848e-ca207d8be60c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a68f502-cd60-4c1e-8916-5177d121fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f190ede-2af7-4ebd-a796-4329dc3e09be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a9831-a02e-4450-b8b0-6b6d2cc04350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed991e5-5960-4510-a28f-e02136a956f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e12f520-e767-44ab-872d-e004c12f4210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa637899-ab03-4c03-a695-9c7ac7dbe507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c906d1-3763-4389-9cdd-59bebf515d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b8ea3b-f99c-468e-ab6d-6f322e1c208d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1f9e36-c02c-41c1-9c05-4adf4929a4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced40bdf-8529-4881-bfc4-837133cdb1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e925c1-246c-4999-a657-50a00e54736e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41282cba-c691-4ba3-be81-6ce605943923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c1c88a-8ba5-478d-84ed-57bf952200b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f7d83-4991-4658-bc09-d2936f636225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6439a730-e2a2-4dde-bc23-773eb246082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c666c53-0f4c-4b57-a4e6-b901843a018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(test_predictions_np), np.unique(y_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7112f12d-995e-41bd-93e8-3e1417d9453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor_np = X_test_tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb54f24-0134-49b3-95b7-46bccec7cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_step = 5\n",
    "# laser_pos = (125 + time[t_step]*laser_speed)* 401/1000  # Laser actual position in true dimension\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3,1, figsize=(12,12), frameon=True)\n",
    "cmap = plt.get_cmap('RdYlGn_r')\n",
    "cmap.set_under('white', alpha=0)\n",
    "hmap1 = ax1.imshow(test_predictions_np[t_step][0], cmap=cmap, vmin=0.5, vmax=1.0, aspect=0.5,  interpolation='quadric')\n",
    "cmap = plt.get_cmap('Wistia')\n",
    "cmap.set_under('white', alpha=0) \n",
    "hmap2 = ax1.imshow(1-test_predictions_np[t_step][0], cmap=cmap, vmin=0.5, vmax=1.5, aspect=0.5, interpolation='quadric')\n",
    "\n",
    "\n",
    "cmap = plt.get_cmap('RdYlGn_r')\n",
    "cmap.set_under('white', alpha=0)\n",
    "hmap1 = ax2.imshow(y_test_np[t_step][0], cmap=cmap, vmin=0.5, vmax=1.0, aspect=0.5,  interpolation='quadric')\n",
    "cmap = plt.get_cmap('Wistia')\n",
    "cmap.set_under('white', alpha=0) \n",
    "hmap2 = ax2.imshow(1-y_test_np[t_step][0], cmap=cmap, vmin=0.5, vmax=1.5, aspect=0.5, interpolation='quadric')\n",
    "\n",
    "cmap = plt.get_cmap('RdYlGn_r')\n",
    "cmap.set_under('white', alpha=0)\n",
    "hmap1 = ax3.imshow(X_test_tensor_np[t_step][0], cmap=cmap, vmin=0.5, vmax=1.0, aspect=0.5,  interpolation='quadric')\n",
    "cmap = plt.get_cmap('Wistia')\n",
    "cmap.set_under('white', alpha=0) \n",
    "hmap2 = ax3.imshow(1-X_test_tensor_np[t_step][0], cmap=cmap, vmin=0.5, vmax=1.5, aspect=0.5, interpolation='quadric')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9f3748-32d0-4f72-b49f-af366f2f1557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1e0b0-5c70-4e47-94c9-1d53b7cc4187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7107b40-b9a9-407a-9b60-981847fd254d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
